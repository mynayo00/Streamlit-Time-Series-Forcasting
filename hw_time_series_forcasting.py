# -*- coding: utf-8 -*-
"""HW_Time Series Forcasting_NADYA ASWARANI

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x4bYQPFoQw-S1I7JwJNKaWyapTsMCHd3
"""

# Import library numerik
# Untuk komputasi numerik seperti log, array, differencing
import numpy as np

# Import library untuk manipulasi dan analisis data dalam bentuk tabel
# Untuk membaca dan mengolah data dalam bentuk DataFrame
import pandas as pd

# Import fungsi ADF (Augmented Dickey-Fuller) Test dari statsmodels untuk uji stasioneritas data time series
# Untuk menguji apakah data time series stasioner
from statsmodels.tsa.stattools import adfuller

# Import matplotlib untuk visualisasi data seperti grafik tren dan forecast
# Untuk membuat grafik
import matplotlib.pyplot as plt
import seaborn as sns

# Import ARIMA model dari statsmodels untuk forecasting data time series yang mengandung tren
# Untuk membangun dan melatih model ARIMA
from statsmodels.tsa.arima.model import ARIMA

# Import Exponential Smoothing (ETS) model dari statsmodels untuk data dengan tren dan musiman
# Untuk model ETS (trend dan seasonality)
from statsmodels.tsa.holtwinters import ExponentialSmoothing

# Import MAPE dari sklearn untuk mengevaluasi akurasi model forecasting
# Untuk menghitung error model (semakin kecil semakin baik)
from sklearn.metrics import mean_absolute_percentage_error, mean_absolute_error, mean_squared_error

# Import joblib untuk menyimpan dan memuat model yang sudah dilatih
import joblib # Untuk save/load model ke file .pkl (agar tidak perlu dilatih ulang)

pd.set_option('display.float_format', '{:.2f}'.format)
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)

from google.colab import drive
drive.mount('/content/drive')

df_1 = pd.read_csv('/content/drive/MyDrive/DS - Dibimbing/Dataset/Time Series/sales_data_january_2019.csv')
df_2 = pd.read_csv('/content/drive/MyDrive/DS - Dibimbing/Dataset/Time Series/sales_data_february_2019.csv')
df_3 = pd.read_csv('/content/drive/MyDrive/DS - Dibimbing/Dataset/Time Series/sales_data_march_2019.csv')
df_4 = pd.read_csv('/content/drive/MyDrive/DS - Dibimbing/Dataset/Time Series/sales_data_april_2019.csv')
df_5 = pd.read_csv('/content/drive/MyDrive/DS - Dibimbing/Dataset/Time Series/sales_data_may_2019.csv')
df_6 = pd.read_csv('/content/drive/MyDrive/DS - Dibimbing/Dataset/Time Series/sales_data_june_2019.csv')
df_7 = pd.read_csv('/content/drive/MyDrive/DS - Dibimbing/Dataset/Time Series/sales_data_july_2019.csv')
df_8 = pd.read_csv('/content/drive/MyDrive/DS - Dibimbing/Dataset/Time Series/sales_data_august_2019.csv')
df_9 = pd.read_csv('/content/drive/MyDrive/DS - Dibimbing/Dataset/Time Series/sales_data_september_2019.csv')
df_10 = pd.read_csv('/content/drive/MyDrive/DS - Dibimbing/Dataset/Time Series/sales_data_october_2019.csv')
df_11 = pd.read_csv('/content/drive/MyDrive/DS - Dibimbing/Dataset/Time Series/sales_data_november_2019.csv')
df_12 = pd.read_csv('/content/drive/MyDrive/DS - Dibimbing/Dataset/Time Series/sales_data_december_2019.csv')

df = pd.concat([df_1, df_2, df_3, df_4, df_5, df_6, df_7, df_8, df_9, df_10, df_11, df_12], ignore_index=True)
df.head()

df.info()

"""# Data Cleaning"""

# step 1 dan 2 : eksplorasi dan pembersiha data
# drop missing rows
df_clean = df.dropna()

# lakukan proses konversi kolom 'ntity Ordered' dan 'Price Each' ke tipe data numerik
# gunakan errors ='coerce' untuk mengubah nlai yang tidak bisa dikonversi menjadi NaN
df_clean['Quantity Ordered'] = pd.to_numeric(df_clean['Quantity Ordered'], errors='coerce') # mengubah niai ke angka dan jika error jadi Nan
df_clean['Price Each'] = pd.to_numeric(df_clean['Price Each'], errors='coerce') # mengubah niai ke angka dan jika error jadi Nan

# hapus baris yang memilki nilai kosong (NaN) setelah konversi ke numerik
#biasanya terjadi jika ada data yang salah format, misal huruf dikolom angka

df_clean = df_clean.dropna(subset=['Quantity Ordered', 'Price Each']) # hanya hapus baris yang kolom ini kosong
df_clean

# ubah kolom 'Order Date' ,emkado format datetime
# gunakan errors='coerce' agar data yang tidak bisa dikonversi (misalnya format salah) menjadi NaT (not a time)

df_clean['Order Date'] = pd.to_datetime(df_clean['Order Date'], errors='coerce')
df_clean.head()

"""# Feature engineering"""

df_clean['Revenue']=df_clean['Quantity Ordered']*df_clean['Price Each']
df_clean.head()

# Setelah yakin datetime, baru ambil tanggal saja (tanpa jam) dan lakukan agregasi
daily_revenue = df_clean.groupby(df_clean['Order Date'].dt.date)['Revenue'].sum().reset_index()

# Rename kolom agar lebih deskriptif
daily_revenue.columns = ['Date', 'Total_Revenue' ]

# Konversi kembali kolom 'Date' ke datetime agar bisa dipakai untuk plotting atau time series model
daily_revenue['Date'] = pd.to_datetime(daily_revenue['Date'])

daily_revenue.head()

# atur kolom 'date' sebagai index dari dataframe
# ambil hanya kolom 'total_revenue' sehinga hasilnya menjadi time series (seri waktu) murni

ts = daily_revenue.set_index('Date')['Total_Revenue']
ts.head()

"""# Uji ADF"""

# lakukan uji ADF (augmented Dickey-fuller) untuk mengecek apakah data time series stasioner atau tidak
# data dikatakan stasionar jika p-value <0.05 (artinya tidak ada akar unit/root)

adf_result = adfuller (ts)
print (f'ADF Statistic: {adf_result[0]}')
print (f'p-value: {adf_result[1]}')

"""p-value lebih dari 0.05 maka data tersebut termasuk ke non-stationary"""

adf_result

# Jika data tidak stasioner (p-value > 0.05), maka kita perlu mentransformasi datanya
# Langkah 1: Terapkan log transform untuk menstabilkan varians (mengurangi efek lonjakan besar)
ts_log = np.log(ts)

# Langkah 2: Terapkan differencing (pengurangan nilai dengan nilai sebelumnya)
# Tujuannya untuk menghilangkan tren agar data menjadi lebih stasioner
ts_log_diff = ts_log.diff().dropna() # .diff() akan menghasilkan NaN di baris pertama, maka gunakan .dropna()

# Lakukan uji ADF lagi setelah transformasi log dan differencing
# Tujuannya untuk memastikan data sudah menjadi stasioner

adf_result_diff = adfuller(ts_log_diff) # Jalankan ADF Test pada data yang telah ditransformasi
print(f'ADF Statistic after diff: {adf_result_diff[0]}') # Statistik ADF baru
print(f'p-value after diff: {adf_result_diff[1]}') # p-value baru, harus 0.05 untuk stasioner

# Pisahkan data menjadi dua bagian: data latih (train) dan data uji (test)
# Di sini, kita gunakan data Februari untuk melatih model
# Dan data Maret digunakan sebagai validasi untuk menguji performa model

train = ts[:'2019-09-30'] # Data latih: sampai akhir Februari
test = ts['2019-10-01':] # Data uji: mulai dari 1 Maret ke depan

# Buat model ARIMA untuk data time series
# Di sini kita gunakan log(train) agar model lebih stabil terhadap variasi besar
# Parameter (1,1,1) artinya: p=1 (Auto REGRESSION), d=1 (differencing), q=1 (MOVING AVERAGE)

model_arima = ARIMA(np.log(train), order=(1,1,1)) # Buat model ARIMA dengan log-transformed data
result_arima = model_arima.fit() # Latih model ARIMA

# Forecast dalam bentuk log selama panjang data uji (misal Maret = 31 hari)
forecast_arima_log = result_arima.forecast(steps=len(test))

# Kembalikan hasil forecast dari skala log ke skala asli dengan fungsi eksponensial
forecast_arima = np.exp(forecast_arima_log) # Ini adalah hasil prediksi revenue harian

# Buat model ETS (Exponential Smoothing) dengan komponen tren dan musiman
# 'trend="add"' artinya kita mengasumsikan adanya tren naik/turun yang bersifat aditif
# 'seasonal="add"' artinya ada pola musiman (berulang) yang juga ditambahkan
# 'seasonal_periods=7' menunjukkan adanya pola mingguan (7 hari)

model_ets = ExponentialSmoothing(train, trend='add', seasonal='add', seasonal_periods=7) # buat model ETS
result_ets = model_ets.fit() # Latih modelnya

# Lakukan prediksi sebanyak panjang data uji
forecast_ets = result_ets. forecast(steps=len(test)) # Hasil prediksi revenue harian dari model ETS

# Hitung MAPE (Mean Absolute Percentage Error) untuk mengukur akurasi kedua model
# Semakin kecil nilai MAPE, semakin akurat prediksi model terhadap data aktual

mape_arima = mean_absolute_percentage_error(test, forecast_arima)
mape_ets = mean_absolute_percentage_error(test, forecast_ets)

# Tampilkan hasil evaluasi dalam format persentase
print(f'MAPE ARIMA: {mape_arima :.2%}') # Misalnya: 8.45%
print(f'MAPE ETS: {mape_ets :.2%}' )

# Bandingkan nilai MAPE dari ARIMA dan ETS
# Pilih model dengan MAPE terkecil sebagai model terbaik

best_model = "ARIMA" if mape_arima < mape_ets else "ETS" # Jika MAPE ARIMA lebih kecil, pilih ARIMA, jika tidak pilih ETS

# Tampilkan model terbaik berdasarkan evaluasi MAPE
print(f"Model terbaik berdasarkan MAPE adalah: {best_model}")

"""# Question

##**Q1**

Hitunglah total revenue, jumlah order, dan jumlah barang yang terjual sepanjang tahun 2019. Selain itu, hitung rata-rata jumlah barang yang dibeli per transaksi dan rata-rata spending per transaksi.
"""

df_clean.head()

total_revenue = df_clean['Revenue'].sum()
total_order = df_clean['Order ID'].nunique()
total_quantity = df_clean['Quantity Ordered'].sum()
avg_quantity_per_order = df_clean.groupby('Order ID')['Quantity Ordered'].sum().mean()
avg_spending_per_order = df_clean.groupby('Order ID')['Revenue'].sum().mean()

print(f'Total revenue pada tahun 2019 adalah sebesar {total_revenue:,} dengan total order {total_order:,} dan jumlah barang {total_quantity:,}. Rata-rata jumlah barang yang dibeli per transaksi adalah {avg_quantity_per_order:.2f} dan rata-rata spending per transaksi adalah {avg_spending_per_order:.2f}')

"""## **Q2**

Hitunglah jumlah order dan GMV yang diperoleh dengan rentang waktu berikut:
- Harian
- Mingguan
- Bulanan

Catatan: GMV dihitung berdasarkan total spending yang dilakukan customer dengan memperhitungkan semua biaya dan diskon yang dibayarkan customer.
"""

df_clean['GMV'] = df_clean['Revenue']

"""### Monthly"""

def konversi_nama_bulan(x):
  if x == 1:
    return "January"
  elif x == 2:
    return 'February'
  elif x == 3:
    return 'Maret'
  elif x == 4:
    return 'April'
  elif x == 5:
    return 'May'
  elif x == 6:
    return 'June'
  elif x == 7:
    return 'July'
  elif x == 8:
    return 'August'
  elif x == 9:
    return 'September'
  elif x == 10:
    return 'October'
  elif x == 11:
    return 'November'
  else:
    return 'December'

monthly = df_clean.groupby(pd.Grouper(key='Order Date', freq='M')).agg(jumlah_order=('Order ID', 'nunique'),GMV=('GMV', 'sum')).reset_index()
monthly['Month'] = monthly['Order Date'].dt.month
monthly['Month'] = monthly['Month'].apply(konversi_nama_bulan)
monthly = monthly.sort_values(by='GMV', ascending=False)
monthly

best_month, month_gmv_max = monthly.loc[monthly['GMV'].idxmax(), ['Month', 'GMV']]
print(f"Pada tahun 2019, {best_month} menjadi bulan dengan GMV tertinggi dengan total {month_gmv_max:,.0f}")

class trend_rating:
    def trend_data(df_clean, col):
        if col == 'Order ID':
            temp = (
                df_clean.set_index('Order Date')[col]
                .resample('MS')
                .count()
                .to_frame()
            )
        else:
            temp = (
                df_clean.set_index('Order Date')[col]
                .resample('MS')
                .mean()
                .to_frame()
            )
        return temp

    def plot_trend(df_clean, col):
        plt.figure(figsize=(16,4))
        temp = trend_rating.trend_data(df_clean, col)
        sns.lineplot(data=temp, x=temp.index, y=col)
        plt.title(f'Trend of {col} (Daily)')
        plt.xlabel('Day')
        plt.ylabel(col)
        plt.show()

for col in ['Quantity Ordered','Revenue','Order ID']:
   if col not in ['Price Each','GMV']:
       trend_rating.plot_trend(df_clean, col)

"""### Weekly"""

weekly = df_clean.groupby(pd.Grouper(key='Order Date', freq='W')).agg(jumlah_order=('Order ID', 'nunique'),GMV=('GMV', 'sum')).reset_index()

weekly['Week'] = weekly['Order Date'].dt.isocalendar().week
weekly['Week'] = "Week " + weekly['Week'].astype(str)

weekly

weekly.sort_values(by='GMV', ascending=False).head(5)

weekly['Month'] = weekly['Order Date'].dt.month
weekly['Month'] = weekly['Month'].apply(konversi_nama_bulan)
weekly.head()

best_week, week_gmv_max, name_month = weekly.loc[weekly['GMV'].idxmax(), ['Week', 'GMV', 'Month']]
print(f"Pada tahun 2019, {best_week} pada bulan {name_month} menjadi minggu dengan GMV tertinggi dengan total {week_gmv_max:,.0f}")

class trend_rating:
    def trend_data(df_clean, col):
        if col == 'Order ID':
            temp = (
                df_clean.set_index('Order Date')[col]
                .resample('W')
                .count()
                .to_frame()
            )
        else:
            temp = (
                df_clean.set_index('Order Date')[col]
                .resample('W')
                .mean()
                .to_frame()
            )
        return temp

    def plot_trend(df_clean, col):
        plt.figure(figsize=(16,4))
        temp = trend_rating.trend_data(df_clean, col)
        sns.lineplot(data=temp, x=temp.index, y=col)
        plt.title(f'Trend of {col} (Weekly)')
        plt.xlabel('Week')
        plt.ylabel(col)
        plt.show()

for col in ['Quantity Ordered','Revenue','Order ID']:
   if col not in ['Price Each','GMV']:
       trend_rating.plot_trend(df_clean, col)

"""### Daily"""

daily = df_clean.groupby(pd.Grouper(key='Order Date', freq='D')).agg(jumlah_order=('Order ID', 'nunique'),GMV=('GMV', 'sum')).reset_index()
daily['Tanggal'] = daily['Order Date'].dt.strftime("%#d %B %Y")
daily

df_clean.info()

class trend_rating:
    def trend_data(df_clean, col):
        if col == 'Order ID':
            temp = (
                df_clean.set_index('Order Date')[col]
                .resample('D')
                .count()
                .to_frame()
            )
        else:
            temp = (
                df_clean.set_index('Order Date')[col]
                .resample('D')
                .mean()
                .to_frame()
            )
        return temp

    def plot_trend(df_clean, col):
        plt.figure(figsize=(16,4))
        temp = trend_rating.trend_data(df_clean, col)
        sns.lineplot(data=temp, x=temp.index, y=col)
        plt.title(f'Trend of {col} (Daily)')
        plt.xlabel('Day')
        plt.ylabel(col)
        plt.show()

for col in ['Quantity Ordered','Revenue','Order ID']:
   if col not in ['Price Each','GMV']:
       trend_rating.plot_trend(df_clean, col)

best_day, day_gmv_max = daily.loc[daily['GMV'].idxmax(), ['Tanggal', 'GMV']]
print(f"Pada tahun 2019, {best_day} menjadi hari dengan GMV tertinggi dengan total {day_gmv_max:,.0f}")

"""## **Q3**
Tim marketing ingin mengetahui produk apa saja yang paling sering dibeli dalam 1 tahun terakhir. Rencananya, mereka akan mencoba mem-bundling top produk ini untuk meningkatkan penjualan. Identifikasi top 10 produk yang membawa revenue terbesar dalam 3 bulan terakhir dan produk apa saja yang bisa di-bundling berdasarkan hasil analisis.

"""

df_clean.head()

"""### Best Revenue Product 2019"""

gb_product_revenue = df_clean.groupby('Product')['Revenue'].sum().reset_index()
top_10_product = gb_product_revenue.sort_values(by ='Revenue', ascending=False).head(10)
top_10_product

product_list = top_10_product['Product'].tolist()
print("Top 10 produk yang memberikan revenue terbesar adalah:\n" + "\n".join(f"- {p}" for p in product_list))

"""### Best Revenue Product 2019 in 3 Month"""

max_order_date = df_clean['Order Date'].max()
three_months_before = max_order_date - pd.DateOffset(months=3)
three_months_before

df_clean.head()

product_info = df_clean[['Product', 'Price Each']]
product_info = product_info.drop_duplicates().sort_values(by='Product', ascending=False)
product_info

product_3month = df_clean.loc[df_clean['Order Date'] >= three_months_before]
product_3month = product_3month.groupby('Product')[['Revenue', 'Quantity Ordered']].sum().reset_index()
product_3month = product_3month.merge(product_info, on='Product', how='left')
product_3month

top_product_3month = product_3month.sort_values(by ='Revenue', ascending=False).head(10)
top_product_3month

product_list_3month = top_product_3month['Product'].tolist()
print("Top 10 produk yang memberikan revenue terbesar dalam 3 bulan terakhir adalah:\n" + "\n".join(f"- {p}" for p in product_list_3month))

low_product_3month = product_3month.sort_values(by ='Revenue', ascending=True).head(10)
low_product_3month

"""Rekomendasi produk bundling:
- Produk komplementari dari best product 3 bulan terakhir seperti
  - Apple Airpods Headphones
    - Macbook Pro Laptop
    - iPhone
  - Bose SoundSport Headphones
    - 27in FHD Monitor
    - 34in Ultrawide Monitor
    - 27in 4K Gaming Monitor
    - Google Phone
    - ThinkPad Laptop
- Produk best-low selling
  - Wired Headphones - semua produk best selling
  - USB-C Charging Cable - semua produk best selling
  - Lightning Charging Cable - iPhone
-  Produk sesuai kebutuhan
    - Produktif : Laptop (Macbook Pro Laptop atau ThinkPad Laptop) dengan monitor (kategori best ataupun low selling)

## **Q4**

Identifikasi top 5 kota yang memiliki order terbanyak dan 5 kota yang memiliki total dan rata-rata spending terbesar.
"""

df_clean['City'] = df_clean['Purchase Address'].str.split(",").str[1].str.strip()

"""### Top 5 City by total order"""

top_city_order = df_clean['City'].value_counts().to_frame().reset_index()
top_city_order = top_city_order.sort_values(by='count', ascending=False).head(5)

city_list = top_city_order['City'].tolist()
print("Top 5 kota yang memiliki orderan terbanyak:\n" + "\n".join(f"- {p}" for p in city_list))

"""### Top 5 City by total and average revenue"""

top_city_revenue = (df_clean.groupby('City').agg(Total_Revenue =('Revenue', 'sum'), Avg_Revenue=('Revenue', 'mean')).reset_index())
top_city_revenue

top_city_revenue_total = top_city_revenue.sort_values(by='Total_Revenue', ascending=False).head(5)
top_city_revenue_total

city_list = top_city_revenue_total['City'].tolist()
print("Top 5 kota yang memiliki total revenue tertinggi:\n" + "\n".join(f"- {p}" for p in city_list))

top_city_revenue_avg = top_city_revenue.sort_values(by='Avg_Revenue', ascending=False).head(5)
top_city_revenue_avg

city_list = top_city_revenue_avg['City'].tolist()
print("Top 5 kota yang memiliki rata-rata revenue tertinggi:\n" + "\n".join(f"- {p}" for p in city_list))

city_info = df_clean['City'].unique()

city_info

"""- San Francisco menjadi kota dengan total revenue tertinggi dan jumlah order terbanyak.
- Atlanta memiliki rata-rata revenue per order tertinggi

## **Q5**
Tim marketing ingin mengetahui kapan penjualan mencapai titik tertinggi sehingga mereka bisa merancang strategi marketing. Analisis pada rentang jam berapa penjualan terjadi secara aktif (rush hour).
"""

df_clean['Time'] = pd.to_datetime(df_clean['Order Date']).dt.time
df_clean.head()

def categorize_time(x):
  if x>= pd.to_datetime('05:00:00').time() and x < pd.to_datetime('12:00:00').time():
    return 'Pagi'
  elif x >= pd.to_datetime('12:00:00').time() and x <pd.to_datetime('15:00:00').time():
    return 'Siang'
  elif x >= pd.to_datetime('15:00:00').time() and x <pd.to_datetime('18:00:00').time():
    return 'Sore'
  else:
    return 'Malam'

df_clean['Time Categorize'] = df_clean['Time'].apply(categorize_time)

time_sell = df_clean.groupby('Time Categorize')[['Revenue','Quantity Ordered']].sum().reset_index()
time_sell = time_sell.sort_values(by='Revenue', ascending=True)
time_sell

total_transaksi_time = df_clean.groupby('Time Categorize').agg(Jumlah_Transaksi=('Order ID', 'count')).reset_index()
total_transaksi_time

time_sell = time_sell.merge(total_transaksi_time, on='Time Categorize', how='left')

def categorize_range(x):
    if x == 'Pagi':
        return '05.00 s.d. 12.00'
    elif x == 'Siang':
        return '12.00 s.d. 15.00'
    elif x == 'Sore':
        return '15.00 s.d. 18.00'
    else:
        return '18.00 s.d. 05.00'

time_sell['Range_Time'] = time_sell['Time Categorize'].apply(categorize_range)
time_sell

best_time, time_revenue, time_transaksi, time_range = time_sell.loc[time_sell['Revenue'].idxmax(), ['Time Categorize', 'Revenue', 'Jumlah_Transaksi', 'Range_Time']]

print(f"Pada tahun 2019, customer banyak melakukan transaksi pada {best_time} ({time_range}) hari dengan total revenue sebesar {time_revenue:,.0f} dan total transaksi {time_transaksi:,.0f} ")

"""## **Q6**

Buatlah model forecasting menggunakan model yang sudah diajarkan pada materi untuk memprediksi jumlah visitor dan jumlah transaksi untuk 1 bulan ke depan dengan data harian. Anda diharuskan setidaknya membuat 2 model untuk selanjutnya diambil model terbaik berdasarkan MAPE terkecil. Berikan rekomendasi apa yang perlu dilakukan tim bisnis berdasarkan hasil forecasting dan analisis Anda.
"""

# Forecast ulang selama 31 hari (1 bulan ke depan) menggunakan model terbaik yang sudah ditentukan
steps_ahead = 31  # Prediksi 30 hari ke depan

if best_model == "ARIMA":
    # Fit ulang ARIMA ke seluruh data latih
    tuned_arima_model = ARIMA(np.log(train), order=(1,1,1)).fit()

    # Prediksi 30 hari ke depan dalam skala log
    forecast_log = tuned_arima_model.forecast(steps=steps_ahead)

    # Kembalikan hasil ke skala asli
    forecast = np.exp(forecast_log)

else:
    # Fit ulang ETS ke seluruh data latih
    tuned_ets_model = ExponentialSmoothing(train, trend='add', seasonal='add', seasonal_periods=7).fit()

    # Prediksi 30 hari ke depan
    forecast = tuned_ets_model.forecast(steps=steps_ahead)

forecast.index = pd.date_range(start='2020-01-01', periods=steps_ahead, freq='D')

# Tampilkan hasil forecast
print(forecast)

# Mengubah hasil prediksi time series menjadi DataFrame biasa dengan kolom terpisah
# forecast_april awalnya berupa pandas Series, dengan index = tanggal dan value = revenue
# .to_frame() akan mengubah Series menjadi DataFrame
# .reset_index() akan memindahkan index (tanggal) ke kolom biasa agar lebih mudah dilihat/digunakan
forecast_januari_2020 = forecast

forecast_2020_df = forecast_januari_2020.to_frame().reset_index()

# Ganti nama kolom agar lebih deskriptif dan rapi
forecast_2020_df.columns = ['Date', 'Forecasted_Revenue']

# Tampilkan beberapa baris teratas sebagai preview
forecast_2020_df

forecast_2020_df.head(1)

# Pastikan forecast_google_april sudah dalam bentuk DataFrame
forecast_2020_df.columns = ['Date', 'Forecasted_Revenue']

# Ekspor hasil prediksi ke file Excel
forecast_2020_df.to_excel("forecast_january_2020.xlsx", index=False)

# Tampilkan konfirmasi
print("File 'forecast_january_2020.xlsx' berhasil disimpan.")

joblib.dump(best_model, 'model_terbaik.pkl')

# Load model dari file
model_loaded = joblib.load('model_terbaik.pkl')

"""# Reflection Questions

## **Q1**

Mengapa penting menganalisis tren waktu (trend & seasonality) sebelum membuat model forecasting, dan bagaimana pengabaian pola musiman bisa memengaruhi keputusan bisnis? Jelaskan dengan contoh bagaimana prediksi yang mengabaikan seasonality dapat menyebabkan salah perencanaan stok atau kampanye pemasaran.

- Analisis tren dan seasonality penting karena keduanya memengaruhi pola data. Jika pola musiman diabaikan, maka model forecasting berisiko tidak sesuai dengan aktual.

- Pengabaian pola musiman dapat membuat bisnis kehilangan momentum dan kesempatan untuk mengambil keputusan di waktu yang tepat

- Contoh: saat Lebaran, permintaan kue kering meningkat. Jika bisnis memahami pola ini, mereka bisa menyiapkan bahan baku lebih banyak dibandingkan hari biasa untuk memenuhi tingginya permintaan

## **Q2**

Bagaimana metrik agregasi berbeda (harian, mingguan, bulanan) membantu tim bisnis dalam mengambil keputusan operasional dan strategis? Berikan contoh keputusan yang lebih tepat dibuat dengan data mingguan dibandingkan data harian, atau sebaliknya.

Metrik agregasi harian, mingguan, dan bulanan membantu tim bisnis melihat data dari sudut pandang yang berbeda sesuai kebutuhan pengambilan keputusan

Data Harian
- Memberikan data detail
- Dibutuhkan untuk keputusan operasional cepat
- Case : mengatur shift karyawan, menambah stok harian, atau merespons lonjakan traffic mendadak
- Contoh: restoran online melihat pesanan melonjak setiap Jumat malam. Maka dari itu perlu ditambah kurir khusus di hari tersebut

Mingguan
- Lebih stabil untuk melihat tren jangka pendek
- Contoh: e-commerce menilai efektivitas iklan maka data mingguan menunjukkan tren konsisten peningkatan transaksi, bukan fluktuasi harian

Bulanan
- Memberikan gambaran besar untuk strategi jangka panjang
- Dibutuhkan untuk budgeting, target penjualan, dan evaluasi program besar
- Contoh: perusahaan retail menentukan anggaran promosi bulan depan berdasarkan tren penjualan bulanan
"""